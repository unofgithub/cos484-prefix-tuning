./data2textprefixtune_y_10_act_cat_b=10-e=1_d=0.0_u=no_lr=5e-05_w=0.0_s=22_r=t_m=512_earlystop_o=1_o=1
python gpt2/task_preinit_ablation/run_language_modeling_preinit_ablation.py         --output_dir=./data2textprefixtune_y_10_act_cat_b=10-e=1_d=0.0_u=no_lr=5e-05_w=0.0_s=22_r=t_m=512_earlystop_o=1_o=1         --model_type=gpt2         --model_name_or_path=gpt2-medium         --tokenizer_name=gpt2-medium         --per_device_train_batch_size 10         --per_device_eval_batch_size 10         --save_steps 500000         --num_train_epochs 1         --do_train         --train_data_file=data/e2e_data/src1_train.txt         --do_eval         --line_by_line         --save_total_limit 1         --overwrite_output_dir         --task_mode data2text         --eval_data_file=data/e2e_data/src1_valid.txt          --tuning_mode prefixtune --logging_dir ./runs/data2textprefixtune_y_10_act_cat_b=10-e=1_d=0.0_u=no_lr=5e-05_w=0.0_s=22_r=t_m=512_earlystop_o=1_o=1         --train_embs no --optim_prefix yes --preseqlen 10 --prefix_mode activation --format_mode cat --gradient_accumulation_steps 1 --learning_rate 5e-05 --weight_decay 0.0 --seed 22 --disable_tqdm --mid_dim 512 --init_random table2text --use_dropout no --prefix_dropout 0.0 --objective_mode 1 --evaluate_during_training --eval_steps 5000  --cache_dir gpt2-medium-s3 
